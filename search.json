[
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "assignment 2",
    "section": "",
    "text": "home\n\nDemostration of R plotting functions\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=16)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n# Exercise: Can you generate these charts individually?  Try these functions \n# using another dataset. Be sure to work on the layout and margins\n\nCan you change pch? pch indicates the symbol to use for the points.  Try different cex value?\nCex is used to change the size of an element in the plot. This is using the cex = 1.\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=1) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\nWhat is the first number standing for? The first number of the axis function indicates the side of the plot for the axis to be drawn on. Exercise: Can you generate these charts individually? The charts can be done individually, the par() setting the layout just needs to be removed. Try these functions using another dataset. Be sure to work on the layout and margins *Demonstration using the Happy Planet Index data.\n\n\n[1] \"C:/Users/brist/Documents/WebOut_Stout\"\n\n\n[1] \"C:/Users/brist/Documents/WebOut_Stout\"\n\n\nThis show the normal plot function and the the is a two y axis plot using the lines fucntion to connect the groups.\n\nplot(hpi$HPI, hpi$Life.Expectancy..years., pch=16,col=\"blue4\",\n     xlab = \"Happy Planet Index\", ylab = \"Life Expectacy\")\ntext(50, 60, \"Happy Planet Index\\nverse\\nLife Expectancy\")\n\n\n\n\n\n\n\nx &lt;- hpi$HPI\ny1 &lt;- hpi$Ladder.of.life..Wellbeing...0.10.\ny2 &lt;- hpi$Carbon.Footprint..tCO2e.\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 45))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, max(x), 5)) # What is the first number standing for?\naxis(2, at=seq(0, 50, 5))\naxis(4, at=seq(0, 50, 5))\nbox(bty=\"u\")\nmtext(\"HPI\", side=1, line=2, cex=0.8)\nmtext(\"Wellbeing\", side=2, line=2, las=0, cex=0.7)\nmtext(\"Carbon Footprint\", side=4, line=2, las=0, cex=0.7)\ntext(40, 30, \"Happy Planet Index\", cex=1.5)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\nThese charts are a histogram with a density curve for the life expectancy and a stack bar chart using Continent and Index rank as categorical values to show population.\n\nlE &lt;- hpi$Life.Expectancy..years.\nlE &lt;- lE[!is.na(lE)]\npar(mar=c(4.5, 4.1, 3.1, 1))\nhist(lE, probability = TRUE, ylim=c(0, 0.06), \n     col=\"honeydew3\", main = \"Life Expectancy in Years\",\n     xlab = \"Life Expectancy\")\nabline(v = mean(lE), col='firebrick', lwd = 3)\nlines(density(lE), col = 'springgreen3', lwd = 2)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n\ntt &lt;- matrix(nrow = 3)\n# Create the bar chart\nc1 = hpi[hpi$Continent == 1, ]\nc2 &lt;- aggregate(c1$Population..thousands., list(c1$rank3), FUN=\"mean\")\nc3 &lt;- as.vector(c2$x)\ntt &lt;- cbind(tt[0], c3)\nfor (n in 2:8){\n  c1 = hpi[hpi$Continent == n, ]\n  c2 &lt;- aggregate(c1$Population..thousands., list(c1$rank3), FUN=\"mean\")\n  if (n == 5 | n == 6){\n    tt &lt;- cbind(tt, append(as.vector(c2$x), 0, 0))\n  } else {\n    tt &lt;- cbind(tt, as.vector(c2$x))\n  }\n}\n\ncontm = list(\"Latin\\nAmerica\", \"North\\nAmerica\", \"West\\nEurope\", \"Middle\\nEast &\\nN. Africa\", \"Africa\", \"East Asia\", \"C. Asia\", \"South\\nAsia\")\n\npar(mar=c(6, 5, 4, 2)) \nmidpts &lt;- barplot(tt, main = \"Population by Index Rank\", names.arg = contm, ylab = \"\",  las=2, col= c(\"#313131\", \"#5F5F5F\", \"#8D8D8D\"))\ntitle(ylab=\"Pop in Thousands)\", line=4, cex.lab=1)\n#text(rep(midpts, each=5), apply(tt, 2, cumsum) - tt/2,\n#     round(tt/1000, digits = 1), \n#     col=rep(c(\"white\", \"black\"), times=3:2), \n#     cex=0.3)\n\nlegend(2, 800000, legend=c(\"Top Third\", \"Middle Third\", \"Bottom Third\"),  \n       fill = c(\"#313131\", \"#5F5F5F\", \"#8D8D8D\"))\n\n\n\n\n\n\n\n#par(mar=c(10, 7, 7, 10)) \n\nLastly, these are a 3d chart of the country ranks and a pie chart of population by continent.\n\npar(mfrow=c(1, 2))\n\n# Generate data for the plot\nx &lt;- hpi$HPI.rank\ny &lt;- x\nf &lt;- function(x, y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\nop &lt;- par(bg = \"white\")\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = \"lightblue\",\n      main = \"3D plot of country rank\")\n\n\npop &lt;- aggregate(hpi$Population..thousands., list(hpi$Continent), FUN=\"mean\")\n\npar(mar=c(0, 1, 1, 3), xpd=FALSE, cex=0.5)\npie(pop$x, col = gray(seq(0.3,1.0,length=6)), labels = cont,cex=1, \n    main = \"Population by Continent\")"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment1",
    "section": "",
    "text": "home\n\nAssignment 1\n\n1. Anscombe’s examples\n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n\n\n\n\n\n\n\n\n\n\n2. Generative art\nDelcan, Pablo. 2024. “I’m Just a Human Sitting in Front of a Stack of Blank Paper, Sketching as Fast as I Can” The New York Times, September 5. https://www.nytimes.com/2024/09/05/opinion/ai-art-model-prompt-brush.html.\nKent, Charlotte. 2020. “The Game of Life - Emergence in Generative Art” The Brooklyn Rail, September 8. https://brooklynrail.org/2020/09/artseen/The-Game-of-Life-Emergence-in-Generative-Art/.\nNayyar, Rhea. 2024. “Real Photographer Beats Out Robots in AI Art Competition” Hyperallergic, June 16. https://hyperallergic.com/925633/real-photographer-beats-out-robots-in-ai-art-competition/.\nPearson, Matt. 2011.”generative art a practical guide using processing” Shelter Island, NY: Manning Publishing Co.\n\n\n3. Fall export\nColor: chocolate3\n\n\n\nFall\n\n\n\n\n4. Critique of a chart\n\n\n\nTexas\n\n\nThis article uses stacked area charts to show the percent of electricity produced by type for each state. All share similar issues, but I selected Texas as the example to look at here. For the axes, their main problem is that the text is quite small especially for the percentages and in comparison to the labels of the energy types. An additional issue with the percentages is that while both sides run from 0-100%, the labels are not consistent between the left and the right and as they lack line markings on the sides the labels are difficult to interpret. For the chart itself, the issue is that the shapes are unclear and it is rather more difficult to distinguish different years. The decision to do this likely came from the animation of the different charts at the beginning of the article. However, this choice hampers the charts when they are used outside of the animation. The charts could also use a grid to help distinguish shapes and make the vertical axes cleaner.\nPopovich, Nadja. 2024. “How Does Your State Make Electricity?” New York Times, August 2. https://www.nytimes.com/interactive/2024/08/02/climate/electricity-generation-us-states.html."
  },
  {
    "objectID": "assignment1.html#small-header",
    "href": "assignment1.html#small-header",
    "title": "Assignment1",
    "section": "small header",
    "text": "small header\nreal small\n\nfirst\nsecond Testing of assignment page"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Brennan Stout TA & GIS Doctoral Student at UTD\nPortfolio"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brennan Stout",
    "section": "",
    "text": "Website for GISC 6356.\nAbout Assignment 1 Assignment 2 Assignment 3 Assignment 4 Assignemnt 5 Blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "home\n\nThe Trap of Big Data\nBig data anlaytics pitfall Overfitting and overparameterization\nIn the world of big data, the Google Flu Trends (GFT) is a well know failure to point at as a testament to the weakness of big data and the pitfall those who study it can fall into. For the GFT itself, this was an attempt to predict outbreaks of flu-like illnesses through the analysis of terms being searched by individuals. While the ability to predict outbreaks with this trend model worked for a time, it began to become less consistent with the CDC data on when the outbreaks actually were. The article proposes a couple different reasons as to why these errors began to appear and why these errors show up in other big data projects. First among these reasons, is that of the perception of big data as the ultimate form of data analysis. That is, with enough data the needs of modeling, data validation, and other more traditional means of data analysis are not need. What this can result in, is a dataset that while massive and collected in seemingly a logical way does not have the hallmarks of an intentional dataset built to do analysis on. This brings us to the second reason, with a dataset built from inputs such as internet posts or other such fallible means to get a model that fits well is a challenge but possible. However, that possibility must be tempered by a full and complete understanding of the data and that different models while appearing to fit well could be concealing errors or tends to run counter to the perceived results. This, over fitting was another of the factor in the GFT issues, their 50 million search terms were fitting to only 1152 data points and while there was reasoning behind those terms that were precluded from the model some were highly correlated with the CDC’s data and likely should have been taken as a sign of overfitting. To compound this particular issue was that of the platform being used to gather this data, that being an ever changing ever evolving search engine. While such a problem is not necessarily unique to only large search engines as any data collection method may undergo updates or refinements over their lifetime, the extent to which large search engines may shift is difficult to ascertain and even more so model the impacts of. And finally, the third and final issue with the GFT and big data projects, that of reproducibility a cornerstone of science as we understand it. These big data projects with their oversized datasets and propensity towards proprietary collection methods can produce results which cannot be reproduced. This was the case with the GFT, as the researchers obscured the search terms being looked for, combined with the inability for Google to release the full dataset means this project was irreplicable and as a result with the deterioration of the predictive power of the model the GFT at this point only has real use as a warning of the pitfalls of big data.\n\n\nDr Edward Tufte The Future\nThe main take away from this talk I feel is that while the mission and objectives of data visualization are fairly clear, the achieving of them are far and away more difficult. A large part of this gap is that the in order to communicate data most effectively one needs to deeply understand not just the data but also how you as a researcher operate and understands yourself. According to Dr. Tufte, the problems arising from an incomplete understanding can take several forms. Perhaps the simplest presented was that of data error, inconsistent formats and incomplete datasets that go unrecognized through the analysis. On the part of the researcher, bias can arise from things like overfocusing on a particular subset or a desire to adjust the fit of the model slightly. These areas of bias can also be externally introduced, through the pressure of sponsors or even the understanding that the method being used is the “best practice”. All these can cause bias or error unless accounted for and accounted for well. Dr. Tufte’s stress this significantly, that humans are fallible, and analysis will follow especially when what is being researched are humans. However, if the one is able to reach the stage of communicating the results of the research to another person the difficulty now comes from choosing from among the myriad of methods to show, tell, or make clear which one to use and advancements in technology have only made this decision even harder. As is put forward in the presentation, data visualization has the purpose of assisting the reasoning of the viewer about what is being shown. For this reason, visualizations are always to a degree unique to the dataset and the person behind it. Even for the same data, a different person will understand it differently and wish to place the focus of the visualization on what they want to communicate from that understanding. There are however, if not better than more effective means of doing that communication, Dr. Tufte highlights elements such as contrasts, comparisons, and the use of space. There should be no extraneous elements or choices made for the sake of simplicity alone. This strikes at the central point of his talk, that excellence does not come from half measures. Communication of any form, operates at the highest level when the one doing the communicating has the knowledge of the subject and the understanding of precisely which elements of that knowledge must be comprehended by the other party. And while those elements are ambiguous and the subject unspecified, when it is used in the realm of science the understood goal should be that the truth is being sought in good faith and what is being communicated aligns as best as is able with that truth.\n\n\nComparison of Visualization Styles\nWithin data visualization, there are a vast range of ways to tell stories with data. And while there do tend to be best practices or optimal strategies for doing so, there is still a great deal of room for creativity and style. An example of the differences in style between schools of data visualization are those of Colin Ware and Leland Wilkinson. For Wilkinson, with the book The Grammer of Graphics he presents an introduction to how and why statistical graphics are done. As a result, while there are a great many examples of graphics and graphical elements, most of them are comparatively simple. This is a book on how to perform visualization and the graphics are built in such a way that they can easily be understood and replicated. Use of colors is limited to only when necessary and all other graphics use the other aesthetic attributes. As presented by the book, these are Form, Color, Texture, and Optics with the Form and brightness of the Color attribute being the seeming to be the most used in the book. In contrast, Ware and his book Information Visualization: Perception for Design places more focus on how the graphics is understood by the viewer and the importance of how perception works. This is a book on the why visualization is important, the parts of a graphic that are of the most use and how can one tell a story with them. What arises from this focus is the use of much more complex graphics using many more individual elements and methods than would appear in Wilkinson’s work. These graphics are for the demonstration of a theoretical topic, where the story is told in the graphic itself and not in the creation of it as would be the case in Wilkinson. Many of the graphics also use networks, connections and a degree of movement through the plots to achieve more complete and self-contained graphics. While these are better to fully communicate the information in question, they do lose a degree of understandability in that complexity while addressed in the book can still be off putting. To contrast, while Wilkinson’s work tends to come across as more understandable, he does not focus on since high-minded topics and instead works off the goal of providing a pathway in how to effectively and fully live into those topics as presented by Ware. Both books have great value in this, Ware has the why visualization is a necessary part of not just science communication but also any digital avenue of communication while Wilkinson has the how to build those visualizations in meaningful ways and the value behind each element in the toolbox of the scientist.\nWilkinson, L. 2005. The grammar of graphics (2nd ed.). Springer Science. Chicago, IL.\nWare, C. 2021. Information visualization: perception for design (Fourth edition.). Morgan Kaufmann. Cambridge, MA.\n\n\nHOW TO IMPROVE YOUR RELATIONSHIP WITH YOUR FUTURE SELF\nThis paper focuses on the underlying parts of data analysis, namely those of coding. Bowers thesis of this paper being that all data analysis stems from programming and that programming in of itself is a means of communicating data both to those using the data but also the scientists themselves down the line. This involves both making the code clear and functional, but also future proofing the code. That is evaluating the libraries and functions being that that they will retain their functions for the full lifespan of the project and the use of comments to fully, completely, and in a proper style communicate what the code is doing and what the full flow of the project is. And while this works to keep the code clear, at the level above there is keeping the file organization clear and logical. To keep this organization, clear in the future, this should also be paired with documentation of some form. While these may seem like the obvious best practices, they do take time and effort to do well and may be forgotten or overlooked in the heat of the moment. When this occurs, it might take massively more time to correct that lack than it would have at the time when that information was still in the scientists’ mind. Regarding the data, while there are a million ways to get data into a project the number of ways that work to clearly and consistently preserve the data for both future use in the code but in a way that is accessible for other scientists to perform repetition research on the data is far more limited. It is important to note that this area falls less into the area of best practice and more into the responsibility of the researcher to do good science and provide the necessary proof of the results found, and the data used in the test is an integral part of that. Fortunately, as technology has advanced this has become easier to do, placing data into a public facing repository with proper documentation like GitHub is increasingly easy even for massive datasets. With these, it gives the additional benefits of version tracking and ease of coordination between team members. As the paper lays out, science is about communication, the best papers do not break ground in their process necessary but in how they communicate that process. That communication goes to every level of the project as well, since while the final project may be the end goal and ostensively the most important ever step along the way is communication to both of you but also a record of what the work is done, and that the conclusion drawn in the final product are both valid and meaningful. To be open, clear, and having your work checked is the basic assumption of science.\n\n\nBach, Fugue in A minor, BWV 904 Visualized\nVisualizations of music are interesting in that they must give visual form to that which exists only in space a time. This particular one breaks the music down into elements which can be visualized like time, volume, notes, and instruments. Movement in this visualization is provided with the movement of time along the x axis while the notes are set along the y. Volume, instruments, and how long notes are held are show by the size and color of the elements moving along the time scale with the currently “played” notes highlighted in the center of the screen. While this works well for this piece of music, the options for this style of visualization to be used for others may not work well. The time scale for this is consistent, there is little ability to change the time signature being used as it is not represented beyond simple movement. Another problem is the musical scale being used, as it seems to be equivalent to that of a piano and as such is constrained to that scale and the half and whole step movements which it can represent. The line and circle elements may better show the movement between notes, but they too are also constrained to this scale. Finally, the lack of a legend hampers the ability of the user to deduce what elements represent, what sound and that deduction can only be done in motion. The ability of this visualization to show the music is exceptional, but it requires the attention of the viewer and not a little foreknowledge of what is being shown to be fully understood.\n\nDemand for office space\nWhile this is a fairly simple bar chart, it does quite well in demonstrating the trend in the data. Between the well spaced labels and blue color scheme the chart is very readable.\n\nlibrary(\"ggplot2\")\n\np &lt;- ggplot(data=df,aes(x=reorder(city,ord),y=stock,fill=cont)) +\n  geom_bar(stat=\"identity\",width=.4) + \n  geom_text(data=df,aes(x=city,y=tt, label = city),size=3,color=\"white\",nudge_x=.5) +\n  geom_errorbar(aes(ymin = stock,ymax=tt-.076),width=0,color=\"white\") +\n  geom_text(data=df,aes(x=city,y=stock, label = stock),vjust=1.5,size=3,color=\"black\") +\n  scale_fill_manual(values = c(\"white\",\"#0099ff\",\"#53baff\"))+\n  ylim(0,2.8) + ggtitle(\"Increase in housing stock if all excess office space was converted into residences by 2030,%\")\n\np + theme(axis.line=element_blank(),axis.text.x=element_blank(),\n          axis.text.y=element_blank(),axis.ticks=element_blank(),\n          axis.title.x=element_blank(), legend.title = element_blank(),\n          axis.title.y=element_blank(),legend.position.inside=c(1,1),\n          legend.justification = c(1,1), plot.title=element_text(size=11,face=\"bold\"),\n          #panel.background=element_blank(),\n          panel.background = element_rect(fill = \"#00004d\"),\n          panel.border=element_blank(),panel.grid.major=element_blank(), panel.grid.minor=element_blank(),plot.background=element_blank()) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nassignment 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrennan Stout\n\n\n\n\n\n\n\n\n\n\n\nBrennan Stout\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment1\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "home"
  },
  {
    "objectID": "assignment3.html#rerun-murrell01-chart-scatterplot",
    "href": "assignment3.html#rerun-murrell01-chart-scatterplot",
    "title": "Assignment 3",
    "section": "Rerun murrell01, chart: scatterplot",
    "text": "Rerun murrell01, chart: scatterplot\n\n#vector for x\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\n#vector for y\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\n#vector for y2\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins (4,4,2,4) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new() #start a new plot\nplot.window(range(x), c(0, 6)) #set the x and y range for the plot window\nlines(x, y1) #add a line\nlines(x, y2) #add a line\npoints(x, y1, pch=16, cex=2) #add points along the line of type 16 size 2 \npoints(x, y2, pch=21, bg=\"white\", cex=2)#add white points along the line of type 21 size 2 \npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\") #set the colors of the axes and labels\naxis(1, at=seq(0, 16, 4)) #for the bottom axis set marks to 16 at intervals of 4\naxis(2, at=seq(0, 6, 2)) #for the left axis set marks to 6 at intervals of 2\naxis(4, at=seq(0, 6, 2)) #for the right axis set marks to 6 at intervals of 2\nbox(bty=\"u\") #set the box shape\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8) #bottom text, line 2, size .8\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8) #left text, angle 0, line 2, size .8\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8) #right text, angle 0, line 2, size .8\ntext(4, 5, \"Bird 131\") #place text at x=4 & y=5"
  },
  {
    "objectID": "assignment3.html#rerun-anscombe01",
    "href": "assignment3.html#rerun-anscombe01",
    "title": "Assignment 3",
    "section": "Rerun anscombe01",
    "text": "Rerun anscombe01\n\n## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\n#View(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nanova(lm1, lm2)\n\nWarning in anova.lmlist(object, ...): models with response '\"y2\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm1, lm3)\n\nWarning in anova.lmlist(object, ...): models with response '\"y3\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm1, lm4)\n\nWarning in anova.lmlist(object, ...): models with response '\"y4\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm2, lm3)\n\nWarning in anova.lmlist(object, ...): models with response '\"y3\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm2, lm4)\n\nWarning in anova.lmlist(object, ...): models with response '\"y4\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm3, lm4)\n\nWarning in anova.lmlist(object, ...): models with response '\"y4\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#all comparisons are the same\npar(mfrow=c(2, 2))\nplot(anscombe$x1,anscombe$y1, pch=16, col=\"orangered1\",\n     main=\"Model 1\")\nabline(coefficients(lm1), lty=1342, lwd=1.6)\nplot(anscombe$x2,anscombe$y2, pch=16, col=\"orangered4\",\n     main=\"Model 2\")\nabline(coefficients(lm2), lty=1342, lwd=1.6)\nplot(anscombe$x3,anscombe$y3, pch=16, col=\"orangered3\",\n     main=\"Model 3\")\nabline(coefficients(lm3), lty=1342, lwd=1.6)\nplot(anscombe$x4,anscombe$y4, pch=16, col=\"orangered2\",\n     main=\"Model 4\")\nabline(coefficients(lm4), lty=1342, lwd=1.6)\n\n\n\n\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"seagreen\", pch = 21, bg = \"greenyellow\", cex = 1.1,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"darkmagenta\", lty=5, lwd=1.4)\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment3.html#finetune-charts",
    "href": "assignment3.html#finetune-charts",
    "title": "Assignment 3",
    "section": "Finetune charts",
    "text": "Finetune charts\n\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\nwindowsFonts(A = windowsFont(\"Cambria Math\"))  \n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"navy\", pch = 23, bg = \"mediumblue\", cex = 1.1,\n       xlim = c(3, 19), ylim = c(3, 13), family=\"A\")\n  abline(mods[[i]], col = \"darkred\", lty=6, lwd=1.4)\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5, family=\"A\")\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment3.html#with-tidyverse",
    "href": "assignment3.html#with-tidyverse",
    "title": "Assignment 3",
    "section": "With tidyverse",
    "text": "With tidyverse\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"ggpubr\")\n\nwindowsFonts(A = windowsFont(\"Cambria Math\"))  \nxm1 &lt;- anscombe[,c(1,5)]\nxm2 &lt;- anscombe[,c(2,6)]\nxm3 &lt;- anscombe[,c(3,7)]\nxm4 &lt;- anscombe[,c(4,8)]\n# Scatter plot with groups\n\npar(mfrow=c(2, 2))\nxg1 &lt;- ggplot(xm1, aes(x = x1, y = y1)) + geom_point(color=\"deeppink3\", size= .9, shape=23, aes(fill = \"lm1\")) + scale_fill_manual(values=c(\"deeppink3\")) + geom_smooth(method='lm',se = FALSE, color=\"mediumpurple\", linetype=\"dotdash\") + theme(text=element_text(size=12,  family=\"serif\"))\nxg2 &lt;- ggplot(xm2, aes(x = x2, y = y2)) + geom_point(color=\"deeppink3\", size= .9, shape=23, aes(fill = \"lm2\")) + scale_fill_manual(values=c(\"deeppink3\")) + geom_smooth(method='lm',se = FALSE, color=\"mediumpurple2\", linetype=\"dotdash\") + theme(text=element_text(size=12,  family=\"serif\"))\nxg3 &lt;- ggplot(xm3, aes(x = x3, y = y3)) + geom_point(color=\"deeppink3\", size= .9, shape=23, aes(fill = \"lm3\")) + scale_fill_manual(values=c(\"deeppink3\")) + geom_smooth(method='lm',se = FALSE, color=\"mediumpurple3\", linetype=\"dotdash\") + theme(text=element_text(size=12,  family=\"serif\"))\nxg4 &lt;- ggplot(xm4, aes(x = x4, y = y4)) + geom_point(color=\"deeppink3\", size= .9, shape=23, aes(fill = \"lm4\")) + scale_fill_manual(values=c(\"deeppink3\")) + geom_smooth(method='lm',se = FALSE, color=\"mediumpurple4\", linetype=\"dotdash\") + theme(text=element_text(size=12,  family=\"serif\"))\nfigure &lt;- ggarrange(xg1, xg2, xg3, xg4,\n                    labels = c(\"1\", \"2\", \"3\", \"4\"),\n                    ncol = 2, nrow = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\nfigure"
  },
  {
    "objectID": "assignment4_wPlot_final.html",
    "href": "assignment4_wPlot_final.html",
    "title": "Assignment 4",
    "section": "",
    "text": "home\n\n# Team GeoVIS\n# Members : Changho Lee, Americo Gamarra, Brennan Stout, Umme Kulsum - Coordinator\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\ndata &lt;- mpg\nfont &lt;- \"serif\"\n\ndata$model&lt;-paste0(toupper(substr(data$model, 1, 1)), substr(data$model, 2, nchar(data$model)))\ndata$manufacturer&lt;-paste0(toupper(substr(data$manufacturer, 1, 1)), substr(data$manufacturer, 2, nchar(data$manufacturer)))\n## 1st Graph\nlibrary(dplyr)\nlibrary(RColorBrewer)\nuniq &lt;- count(unique(data,vars = c(manufacturer, cty)),vars=manufacturer)\nqual_colors &lt;- colorRampPalette(brewer.pal(9, \"PuRd\"))(20)\nnewdata &lt;- data[ which(data$manufacturer=='ford'), ]\ntotal &lt;- sum(uniq$n)\nwidths &lt;- c()\nfor (m in 1:length(uniq$vars)){\n  widths &lt;- append(widths, (uniq$n[m]/total)*100)\n}\nheights = uniq$n\npar( mar= c(4,4,1,1),family=font )\np &lt;- barplot(heights, widths, space=0, \n             col = qual_colors, xlab = \"Percent of cars\", ylab = \"Number of models\")\ntext(x = p, y = heights/2, labels = uniq$vars, cex=ifelse(uniq$vars%in%c(\"Land rover\",\"Lincoln\",\"Mercury\",\"Pontiac\"),0.5,1), srt=90)\naxis(1, seq(0,100,5))\npolygon(c(42,42,72,72), c(25, 30, 30, 25), col=\"snow\")\ntext(57, 28, \"Percent of market\\nby manufacturer\", cex=1)\n\n\n\n\n\n\n\n## 2nd Graph\n# Get the 6 largest manufacturers by count\ntop_makers &lt;- mpg %&gt;%\n  count(manufacturer) %&gt;%\n  top_n(6, n) %&gt;%\n  pull(manufacturer)\n\n# Filter the data to include only the top 6 manufacturers\nmpg_filtered &lt;- mpg %&gt;%\n  filter(manufacturer %in% top_makers)\n\n# Create the faceted histograms for city mpg\nggplot(mpg_filtered, aes(x = cty)) +\n  geom_histogram(binwidth = 2, fill = \"violetred2\", color = \"white\") +\n  facet_grid(class ~ manufacturer) +  # Facet by class and cylinders\n  labs(x = \"City MPG\", y = \"Count\", title = \"City MPG Distribution by Manufacturer, Class, and Cylinder\", family=font) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10,family=font),    # Adjust facet label size\n    panel.spacing = unit(1, \"lines\"),        # Add spacing between facets\n    axis.text.x = element_text(size = 8,family=font),    # Adjust size of x-axis text\n    axis.text.y = element_text(size = 8, family=font),     # Adjust size of y-axis text\n    axis.title.x = element_text(size = 12, family = font),  # Set x-axis label font and size\n    axis.title.y = element_text(size = 12, , family = font),  # Set y-axis label font and size\n    plot.title = element_text(size = 14, family = font, hjust = 0.5),\n    panel.grid = element_blank(),\n    panel.border = element_rect(color = \"gray40\", fill = NA, size = 0.6)\n  )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n## 3rd Graph \nmh &lt;- aggregate(hwy ~ class, data = data, mean)\nmc &lt;- aggregate(cty ~ class, data = data, mean)\n\npar(mfrow = c(1, 2), mar = c(5,6,2,2), mgp = c(4, 1, 0.8), family = font, cex.main = 1.2, \n    cex.lab = 1, font.main = 1, \n    font.lab = 1, font.axis = 1, cex.axis = 0.7)\n\n# mean highway MPG\nbarplot(mh$hwy, names.arg = mh$class,\n        horiz = TRUE, col = \"violetred2\", border = \"black\",\n        xlab = \"Mean MPG\", ylab = \"Type of Vehicle\",\n        xlim = c(0, 35),\n        main = \"Highway\", las = 1)\nabline(v = 0, lwd = 4, col = \"black\") \n\n# mean city MPG\nbarplot(mc$cty, names.arg = mc$class, \n        horiz = TRUE, col = \"white\", border = \"black\", \n        xlab = \"Mean MPG\", ylab = \"\", \n        xlim = c(0, 35),\n        main = \"City\", las = 1)\nabline(v = 0, lwd = 4, col = \"black\") \n\n\n\n\n\n\n\n## 4th Graph\ndata_sum &lt;- mpg %&gt;%\n  group_by(class) %&gt;%\n  summarise(mh = mean(hwy), mc = mean(cty))\n\nggplot(data_sum, aes(x = class)) +\n  geom_col(aes(y = mh), fill = \"violetred2\", width = 0.4, \n           position = position_nudge(x = 0.1)) +\n  geom_col(aes(y = mc), fill = 'white', color = \"black\", \n           width = 0.4, position = position_nudge(x = -0.1)) +\n  labs(x = \"Type of Vehicle\", y = \"Mean Miles per Gallon\", title = \"Mean MPG by Class and Vehicle Type\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),axis.line = element_line(color = \"black\"), \n        axis.text.x = element_text(angle = 45, hjust = 1, family = font), \n        axis.text.y = element_text(family = font),\n        axis.title.x = element_text(family = font, size = 12), \n        axis.title.y = element_text(family = font, size = 12), \n        plot.title = element_text(family = font, \n                                  size = 14, hjust = 0.5))"
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "home"
  },
  {
    "objectID": "assignment5.html#histogram-and-barchart",
    "href": "assignment5.html#histogram-and-barchart",
    "title": "Assignment 5",
    "section": "Histogram and barchart",
    "text": "Histogram and barchart\n\nworlds_fairs &lt;- na.omit(readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-08-13/worlds_fairs.csv'))\n\nRows: 70 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): name_of_exposition, country, city, category, theme, notables\ndbl (8): start_month, start_year, end_month, end_year, visitors, cost, area,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhist(worlds_fairs$visitors, xlab = \"Visitors\", main = \"World's Fair Visitors\", col = \"magenta4\")\n\n\n\n\n\n\n\ntext(\nbarplot(worlds_fairs$attending_countries,names.arg = worlds_fairs$start_year, col = \"steelblue\", ylim = c(0, max(worlds_fairs$attending_countries) * 1.2)),\n  y = worlds_fairs$attending_countries + 1.5, labels = worlds_fairs$country, pos = 3, cex = .7, col = \"black\",srt=90, ylab = \"Attending Countries\", xlab = \"Start Year\", main = \"World's Fair Attending Countries\"\n)\n\n\n\n\n\n\n\nbarplot(worlds_fairs$cost,names.arg=worlds_fairs$start_year, horiz = TRUE, col = \"darkgreen\",las=2,ylab = \"Start Year\", xlab = \"Cost\", main = \"World's Fair Cost\")"
  },
  {
    "objectID": "assignment5.html#piechart",
    "href": "assignment5.html#piechart",
    "title": "Assignment 5",
    "section": "Piechart",
    "text": "Piechart\n\nslices &lt;- table(worlds_fairs$category)\npct &lt;- round(slices/sum(slices)*100)\nlbls &lt;- paste(rev(unique(worlds_fairs$category)), pct)\n# add percents to labels\nlbls &lt;- paste(lbls,\"%\",sep=\"\")\npie(slices,labels = lbls, col=rainbow(length(lbls)),\n   main=\"Pie Chart of World's Fair Categories\")"
  },
  {
    "objectID": "assignment5.html#boxplot",
    "href": "assignment5.html#boxplot",
    "title": "Assignment 5",
    "section": "boxplot",
    "text": "boxplot\n\nboxplot(worlds_fairs$visitors,\nmain = \"World's Fair Visitors\",\nxlab = \"Visitors in millions\",\nylab = \"Fairs\",\ncol = \"orange\",\nborder = \"brown\",\nhorizontal = TRUE,\nnotch = TRUE\n)"
  },
  {
    "objectID": "assignment5.html#scatterplot",
    "href": "assignment5.html#scatterplot",
    "title": "Assignment 5",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nplot(worlds_fairs$start_year, worlds_fairs$visitors, pch=20, col=\"purple4\", ylab = \"Visitos in millions\", xlab = \"Year\", main = \"World's Fair Visitors by Year\")\ntext(worlds_fairs$start_year, worlds_fairs$visitors, labels=worlds_fairs$name_of_exposition, cex= 0.6, pos=3)"
  },
  {
    "objectID": "assignment5.html#ggplot-scatterplot",
    "href": "assignment5.html#ggplot-scatterplot",
    "title": "Assignment 5",
    "section": "ggplot scatterplot",
    "text": "ggplot scatterplot\n\nlibrary(\"ggplot2\")\nggplot(worlds_fairs, aes(x = start_year, y = visitors)) +\n    geom_point(aes(x = start_year, \n                   y = visitors, \n                   color = category),size = 2) +\n  geom_text(aes(start_year,visitors,label=name_of_exposition),position = position_dodge(width=0.9),  size=2) +\n  theme_minimal()+\n  ggtitle(\"Year by Visitors\") +\n  xlab(\"Year\") +\n  ylab(\"Visitors in Millions\")"
  },
  {
    "objectID": "assignment5.html#file-types",
    "href": "assignment5.html#file-types",
    "title": "Assignment 5",
    "section": "File Types",
    "text": "File Types\n\n#pdf(file = 'my_plot.pdf') PDF is a document format\n#jpeg(file = 'my_plot.jpeg') compressable format for images on the internet\n#tiff(file = 'my_plot.tiff') High quality image format\n#svg(file = 'my_plot.svg')  scalable image format\n#bmp(file = 'my_plot.bmp')  uncompressed raster format\n\n#plot to export\n# slices &lt;- table(worlds_fairs$category)\n# pct &lt;- round(slices/sum(slices)*100)\n# lbls &lt;- paste(rev(unique(worlds_fairs$category)), pct)\n# # add percents to labels\n# lbls &lt;- paste(lbls,\"%\",sep=\"\")\n# pie(slices,labels = lbls, col=rainbow(length(lbls)),\n#    main=\"Pie Chart of World's Fair Categories\")\n\n#end export\n#dev.off()\n\npdf\n\njpef\n tiff\n\n\n\ntiff\n\n\nsvg\n\n\n\nsvg\n\n\nbmp\n\n\n\nbmp"
  }
]