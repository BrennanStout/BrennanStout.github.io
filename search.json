[
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "assignment 2",
    "section": "",
    "text": "home\n\nDemostration of R plotting functions\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=16)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n# Exercise: Can you generate these charts individually?  Try these functions \n# using another dataset. Be sure to work on the layout and margins\n\nCan you change pch? pch indicates the symbol to use for the points.  Try different cex value?\nCex is used to change the size of an element in the plot. This is using the cex = 1.\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=1) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\nWhat is the first number standing for? The first number of the axis function indicates the side of the plot for the axis to be drawn on. Exercise: Can you generate these charts individually? The charts can be done individually, the par() setting the layout just needs to be removed. Try these functions using another dataset. Be sure to work on the layout and margins *Demonstration using the Happy Planet Index data.\n\n\n[1] \"C:/Users/brist/Documents/WebOut_Stout\"\n\n\n[1] \"C:/Users/brist/Documents/WebOut_Stout\"\n\n\nThis show the normal plot function and the the is a two y axis plot using the lines fucntion to connect the groups.\n\nplot(hpi$HPI, hpi$Life.Expectancy..years., pch=16,col=\"blue4\",\n     xlab = \"Happy Planet Index\", ylab = \"Life Expectacy\")\ntext(50, 60, \"Happy Planet Index\\nverse\\nLife Expectancy\")\n\n\n\n\n\n\n\nx &lt;- hpi$HPI\ny1 &lt;- hpi$Ladder.of.life..Wellbeing...0.10.\ny2 &lt;- hpi$Carbon.Footprint..tCO2e.\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 45))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, max(x), 5)) # What is the first number standing for?\naxis(2, at=seq(0, 50, 5))\naxis(4, at=seq(0, 50, 5))\nbox(bty=\"u\")\nmtext(\"HPI\", side=1, line=2, cex=0.8)\nmtext(\"Wellbeing\", side=2, line=2, las=0, cex=0.7)\nmtext(\"Carbon Footprint\", side=4, line=2, las=0, cex=0.7)\ntext(40, 30, \"Happy Planet Index\", cex=1.5)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\nThese charts are a histogram with a density curve for the life expectancy and a stack bar chart using Continent and Index rank as categorical values to show population.\n\nlE &lt;- hpi$Life.Expectancy..years.\nlE &lt;- lE[!is.na(lE)]\npar(mar=c(4.5, 4.1, 3.1, 1))\nhist(lE, probability = TRUE, ylim=c(0, 0.06), \n     col=\"honeydew3\", main = \"Life Expectancy in Years\",\n     xlab = \"Life Expectancy\")\nabline(v = mean(lE), col='firebrick', lwd = 3)\nlines(density(lE), col = 'springgreen3', lwd = 2)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n\ntt &lt;- matrix(nrow = 3)\n# Create the bar chart\nc1 = hpi[hpi$Continent == 1, ]\nc2 &lt;- aggregate(c1$Population..thousands., list(c1$rank3), FUN=\"mean\")\nc3 &lt;- as.vector(c2$x)\ntt &lt;- cbind(tt[0], c3)\nfor (n in 2:8){\n  c1 = hpi[hpi$Continent == n, ]\n  c2 &lt;- aggregate(c1$Population..thousands., list(c1$rank3), FUN=\"mean\")\n  if (n == 5 | n == 6){\n    tt &lt;- cbind(tt, append(as.vector(c2$x), 0, 0))\n  } else {\n    tt &lt;- cbind(tt, as.vector(c2$x))\n  }\n}\n\ncontm = list(\"Latin\\nAmerica\", \"North\\nAmerica\", \"West\\nEurope\", \"Middle\\nEast &\\nN. Africa\", \"Africa\", \"East Asia\", \"C. Asia\", \"South\\nAsia\")\n\npar(mar=c(6, 5, 4, 2)) \nmidpts &lt;- barplot(tt, main = \"Population by Index Rank\", names.arg = contm, ylab = \"\",  las=2, col= c(\"#313131\", \"#5F5F5F\", \"#8D8D8D\"))\ntitle(ylab=\"Pop in Thousands)\", line=4, cex.lab=1)\n#text(rep(midpts, each=5), apply(tt, 2, cumsum) - tt/2,\n#     round(tt/1000, digits = 1), \n#     col=rep(c(\"white\", \"black\"), times=3:2), \n#     cex=0.3)\n\nlegend(2, 800000, legend=c(\"Top Third\", \"Middle Third\", \"Bottom Third\"),  \n       fill = c(\"#313131\", \"#5F5F5F\", \"#8D8D8D\"))\n\n\n\n\n\n\n\n#par(mar=c(10, 7, 7, 10)) \n\nLastly, these are a 3d chart of the country ranks and a pie chart of population by continent.\n\npar(mfrow=c(1, 2))\n\n# Generate data for the plot\nx &lt;- hpi$HPI.rank\ny &lt;- x\nf &lt;- function(x, y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\nop &lt;- par(bg = \"white\")\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = \"lightblue\",\n      main = \"3D plot of country rank\")\n\n\npop &lt;- aggregate(hpi$Population..thousands., list(hpi$Continent), FUN=\"mean\")\n\npar(mar=c(0, 1, 1, 3), xpd=FALSE, cex=0.5)\npie(pop$x, col = gray(seq(0.3,1.0,length=6)), labels = cont,cex=1, \n    main = \"Population by Continent\")"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment1",
    "section": "",
    "text": "home\n\nAssignment 1\n\n1. Anscombe’s examples\n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n\n\n\n\n\n\n\n\n\n\n2. Generative art\nDelcan, Pablo. 2024. “I’m Just a Human Sitting in Front of a Stack of Blank Paper, Sketching as Fast as I Can” The New York Times, September 5. https://www.nytimes.com/2024/09/05/opinion/ai-art-model-prompt-brush.html.\nKent, Charlotte. 2020. “The Game of Life - Emergence in Generative Art” The Brooklyn Rail, September 8. https://brooklynrail.org/2020/09/artseen/The-Game-of-Life-Emergence-in-Generative-Art/.\nNayyar, Rhea. 2024. “Real Photographer Beats Out Robots in AI Art Competition” Hyperallergic, June 16. https://hyperallergic.com/925633/real-photographer-beats-out-robots-in-ai-art-competition/.\nPearson, Matt. 2011.”generative art a practical guide using processing” Shelter Island, NY: Manning Publishing Co.\n\n\n3. Fall export\nColor: chocolate3\n\n\n\nFall\n\n\n\n\n4. Critique of a chart\n\n\n\nTexas\n\n\nThis article uses stacked area charts to show the percent of electricity produced by type for each state. All share similar issues, but I selected Texas as the example to look at here. For the axes, their main problem is that the text is quite small especially for the percentages and in comparison to the labels of the energy types. An additional issue with the percentages is that while both sides run from 0-100%, the labels are not consistent between the left and the right and as they lack line markings on the sides the labels are difficult to interpret. For the chart itself, the issue is that the shapes are unclear and it is rather more difficult to distinguish different years. The decision to do this likely came from the animation of the different charts at the beginning of the article. However, this choice hampers the charts when they are used outside of the animation. The charts could also use a grid to help distinguish shapes and make the vertical axes cleaner.\nPopovich, Nadja. 2024. “How Does Your State Make Electricity?” New York Times, August 2. https://www.nytimes.com/interactive/2024/08/02/climate/electricity-generation-us-states.html."
  },
  {
    "objectID": "assignment1.html#small-header",
    "href": "assignment1.html#small-header",
    "title": "Assignment1",
    "section": "small header",
    "text": "small header\nreal small\n\nfirst\nsecond Testing of assignment page"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Brennan Stout TA & GIS Doctoral Student at UTD\nPortfolio"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brennan Stout",
    "section": "",
    "text": "Website for GISC 6356.\nAbout Assignment 1 Assignment 2 Assignment 3 Blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "home\n\nThe Trap of Big Data\nBig data anlaytics pitfall Overfitting and overparameterization\nIn the world of big data, the Google Flu Trends (GFT) is a well know failure to point at as a testament to the weakness of big data and the pitfall those who study it can fall into. For the GFT itself, this was an attempt to predict outbreaks of flu-like illnesses through the analysis of terms being searched by individuals. While the ability to predict outbreaks with this trend model worked for a time, it began to become less consistent with the CDC data on when the outbreaks actually were. The article proposes a couple different reasons as to why these errors began to appear and why these errors show up in other big data projects. First among these reasons, is that of the perception of big data as the ultimate form of data analysis. That is, with enough data the needs of modeling, data validation, and other more traditional means of data analysis are not need. What this can result in, is a dataset that while massive and collected in seemingly a logical way does not have the hallmarks of an intentional dataset built to do analysis on. This brings us to the second reason, with a dataset built from inputs such as internet posts or other such fallible means to get a model that fits well is a challenge but possible. However, that possibility must be tempered by a full and complete understanding of the data and that different models while appearing to fit well could be concealing errors or tends to run counter to the perceived results. This, over fitting was another of the factor in the GFT issues, their 50 million search terms were fitting to only 1152 data points and while there was reasoning behind those terms that were precluded from the model some were highly correlated with the CDC’s data and likely should have been taken as a sign of overfitting. To compound this particular issue was that of the platform being used to gather this data, that being an ever changing ever evolving search engine. While such a problem is not necessarily unique to only large search engines as any data collection method may undergo updates or refinements over their lifetime, the extent to which large search engines may shift is difficult to ascertain and even more so model the impacts of. And finally, the third and final issue with the GFT and big data projects, that of reproducibility a cornerstone of science as we understand it. These big data projects with their oversized datasets and propensity towards proprietary collection methods can produce results which cannot be reproduced. This was the case with the GFT, as the researchers obscured the search terms being looked for, combined with the inability for Google to release the full dataset means this project was irreplicable and as a result with the deterioration of the predictive power of the model the GFT at this point only has real use as a warning of the pitfalls of big data.\n\n\nDr Edward Tufte The Future\nThe main take away from this talk I feel is that while the mission and objectives of data visualization are fairly clear, the achieving of them are far and away more difficult. A large part of this gap is that the in order to communicate data most effectively one needs to deeply understand not just the data but also how you as a researcher operate and understands yourself. According to Dr. Tufte, the problems arising from an incomplete understanding can take several forms. Perhaps the simplest presented was that of data error, inconsistent formats and incomplete datasets that go unrecognized through the analysis. On the part of the researcher, bias can arise from things like overfocusing on a particular subset or a desire to adjust the fit of the model slightly. These areas of bias can also be externally introduced, through the pressure of sponsors or even the understanding that the method being used is the “best practice”. All these can cause bias or error unless accounted for and accounted for well. Dr. Tufte’s stress this significantly, that humans are fallible, and analysis will follow especially when what is being researched are humans. However, if the one is able to reach the stage of communicating the results of the research to another person the difficulty now comes from choosing from among the myriad of methods to show, tell, or make clear which one to use and advancements in technology have only made this decision even harder. As is put forward in the presentation, data visualization has the purpose of assisting the reasoning of the viewer about what is being shown. For this reason, visualizations are always to a degree unique to the dataset and the person behind it. Even for the same data, a different person will understand it differently and wish to place the focus of the visualization on what they want to communicate from that understanding. There are however, if not better than more effective means of doing that communication, Dr. Tufte highlights elements such as contrasts, comparisons, and the use of space. There should be no extraneous elements or choices made for the sake of simplicity alone. This strikes at the central point of his talk, that excellence does not come from half measures. Communication of any form, operates at the highest level when the one doing the communicating has the knowledge of the subject and the understanding of precisely which elements of that knowledge must be comprehended by the other party. And while those elements are ambiguous and the subject unspecified, when it is used in the realm of science the understood goal should be that the truth is being sought in good faith and what is being communicated aligns as best as is able with that truth.\n\n\nComparison of Visualization Styles\nWithin data visualization, there are a vast range of ways to tell stories with data. And while there do tend to be best practices or optimal strategies for doing so, there is still a great deal of room for creativity and style. An example of the differences in style between schools of data visualization are those of Colin Ware and Leland Wilkinson. For Wilkinson, with the book The Grammer of Graphics he presents an introduction to how and why statistical graphics are done. As a result, while there are a great many examples of graphics and graphical elements, most of them are comparatively simple. This is a book on how to perform visualization and the graphics are built in such a way that they can easily be understood and replicated. Use of colors is limited to only when necessary and all other graphics use the other aesthetic attributes. As presented by the book, these are Form, Color, Texture, and Optics with the Form and brightness of the Color attribute being the seeming to be the most used in the book. In contrast, Ware and his book Information Visualization: Perception for Design places more focus on how the graphics is understood by the viewer and the importance of how perception works. This is a book on the why visualization is important, the parts of a graphic that are of the most use and how can one tell a story with them. What arises from this focus is the use of much more complex graphics using many more individual elements and methods than would appear in Wilkinson’s work. These graphics are for the demonstration of a theoretical topic, where the story is told in the graphic itself and not in the creation of it as would be the case in Wilkinson. Many of the graphics also use networks, connections and a degree of movement through the plots to achieve more complete and self-contained graphics. While these are better to fully communicate the information in question, they do lose a degree of understandability in that complexity while addressed in the book can still be off putting. To contrast, while Wilkinson’s work tends to come across as more understandable, he does not focus on since high-minded topics and instead works off the goal of providing a pathway in how to effectively and fully live into those topics as presented by Ware. Both books have great value in this, Ware has the why visualization is a necessary part of not just science communication but also any digital avenue of communication while Wilkinson has the how to build those visualizations in meaningful ways and the value behind each element in the toolbox of the scientist.\nWilkinson, L. 2005. The grammar of graphics (2nd ed.). Springer Science. Chicago, IL.\nWare, C. 2021. Information visualization : perception for design (Fourth edition.). Morgan Kaufmann. Cambridge, MA.\ntext\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nassignment 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrennan Stout\n\n\n\n\n\n\n\n\n\n\n\nBrennan Stout\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment1\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "home"
  },
  {
    "objectID": "assignment3.html#rerun-murrell01-chart-scatterplot",
    "href": "assignment3.html#rerun-murrell01-chart-scatterplot",
    "title": "Assignment 3",
    "section": "Rerun murrell01, chart: scatterplot",
    "text": "Rerun murrell01, chart: scatterplot\n\n#vector for x\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\n#vector for y\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\n#vector for y2\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins (4,4,2,4) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new() #start a new plot\nplot.window(range(x), c(0, 6)) #set the x and y range for the plot window\nlines(x, y1) #add a line\nlines(x, y2) #add a line\npoints(x, y1, pch=16, cex=2) #add points along the line of type 16 size 2 \npoints(x, y2, pch=21, bg=\"white\", cex=2)#add white points along the line of type 21 size 2 \npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\") #set the colors of the axes and labels\naxis(1, at=seq(0, 16, 4)) #for the bottom axis set marks to 16 at intervals of 4\naxis(2, at=seq(0, 6, 2)) #for the left axis set marks to 6 at intervals of 2\naxis(4, at=seq(0, 6, 2)) #for the right axis set marks to 6 at intervals of 2\nbox(bty=\"u\") #set the box shape\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8) #bottom text, line 2, size .8\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8) #left text, angle 0, line 2, size .8\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8) #right text, angle 0, line 2, size .8\ntext(4, 5, \"Bird 131\") #place text at x=4 & y=5"
  },
  {
    "objectID": "assignment3.html#rerun-anscombe01",
    "href": "assignment3.html#rerun-anscombe01",
    "title": "Assignment 3",
    "section": "Rerun anscombe01",
    "text": "Rerun anscombe01\n\n## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\n#View(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nanova(lm1, lm2)\n\nWarning in anova.lmlist(object, ...): models with response '\"y2\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm1, lm3)\n\nWarning in anova.lmlist(object, ...): models with response '\"y3\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm1, lm4)\n\nWarning in anova.lmlist(object, ...): models with response '\"y4\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm2, lm3)\n\nWarning in anova.lmlist(object, ...): models with response '\"y3\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm2, lm4)\n\nWarning in anova.lmlist(object, ...): models with response '\"y4\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm3, lm4)\n\nWarning in anova.lmlist(object, ...): models with response '\"y4\"' removed\nbecause response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#all comparisons are the same\npar(mfrow=c(2, 2))\nplot(anscombe$x1,anscombe$y1, pch=16, col=\"orangered1\",\n     main=\"Model 1\")\nabline(coefficients(lm1), lty=1342, lwd=1.6)\nplot(anscombe$x2,anscombe$y2, pch=16, col=\"orangered4\",\n     main=\"Model 2\")\nabline(coefficients(lm2), lty=1342, lwd=1.6)\nplot(anscombe$x3,anscombe$y3, pch=16, col=\"orangered3\",\n     main=\"Model 3\")\nabline(coefficients(lm3), lty=1342, lwd=1.6)\nplot(anscombe$x4,anscombe$y4, pch=16, col=\"orangered2\",\n     main=\"Model 4\")\nabline(coefficients(lm4), lty=1342, lwd=1.6)\n\n\n\n\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"seagreen\", pch = 21, bg = \"greenyellow\", cex = 1.1,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"darkmagenta\", lty=5, lwd=1.4)\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment3.html#finetune-charts",
    "href": "assignment3.html#finetune-charts",
    "title": "Assignment 3",
    "section": "Finetune charts",
    "text": "Finetune charts\n\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\nwindowsFonts(A = windowsFont(\"Cambria Math\"))  \n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"navy\", pch = 23, bg = \"mediumblue\", cex = 1.1,\n       xlim = c(3, 19), ylim = c(3, 13), family=\"A\")\n  abline(mods[[i]], col = \"darkred\", lty=6, lwd=1.4)\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5, family=\"A\")\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment3.html#with-tidyverse",
    "href": "assignment3.html#with-tidyverse",
    "title": "Assignment 3",
    "section": "With tidyverse",
    "text": "With tidyverse\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"ggpubr\")\n\nwindowsFonts(A = windowsFont(\"Cambria Math\"))  \nxm1 &lt;- anscombe[,c(1,5)]\nxm2 &lt;- anscombe[,c(2,6)]\nxm3 &lt;- anscombe[,c(3,7)]\nxm4 &lt;- anscombe[,c(4,8)]\n# Scatter plot with groups\n\npar(mfrow=c(2, 2))\nxg1 &lt;- ggplot(xm1, aes(x = x1, y = y1)) + geom_point(color=\"deeppink3\", size= .9, shape=23, aes(fill = \"lm1\")) + scale_fill_manual(values=c(\"deeppink3\")) + geom_smooth(method='lm',se = FALSE, color=\"mediumpurple\", linetype=\"dotdash\") + theme(text=element_text(size=12,  family=\"serif\"))\nxg2 &lt;- ggplot(xm2, aes(x = x2, y = y2)) + geom_point(color=\"deeppink3\", size= .9, shape=23, aes(fill = \"lm2\")) + scale_fill_manual(values=c(\"deeppink3\")) + geom_smooth(method='lm',se = FALSE, color=\"mediumpurple2\", linetype=\"dotdash\") + theme(text=element_text(size=12,  family=\"serif\"))\nxg3 &lt;- ggplot(xm3, aes(x = x3, y = y3)) + geom_point(color=\"deeppink3\", size= .9, shape=23, aes(fill = \"lm3\")) + scale_fill_manual(values=c(\"deeppink3\")) + geom_smooth(method='lm',se = FALSE, color=\"mediumpurple3\", linetype=\"dotdash\") + theme(text=element_text(size=12,  family=\"serif\"))\nxg4 &lt;- ggplot(xm4, aes(x = x4, y = y4)) + geom_point(color=\"deeppink3\", size= .9, shape=23, aes(fill = \"lm4\")) + scale_fill_manual(values=c(\"deeppink3\")) + geom_smooth(method='lm',se = FALSE, color=\"mediumpurple4\", linetype=\"dotdash\") + theme(text=element_text(size=12,  family=\"serif\"))\nfigure &lt;- ggarrange(xg1, xg2, xg3, xg4,\n                    labels = c(\"1\", \"2\", \"3\", \"4\"),\n                    ncol = 2, nrow = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\nfigure"
  }
]